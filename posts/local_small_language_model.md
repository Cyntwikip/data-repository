## Locally Deployed Small Language Model

Recently, I deployed a small language model on my trusty 8 GB machine. While it’s not going to beat the giants in raw power, it highlights just how much potential there is in compact models for practical, real-world applications. Here’s a quick demo of TinyLlama, running locally. Excited to share more updates soon!

Looking back to 2019, during my early days at ADB, I was diving into Large Language Models (LLMs) using BERT. Back then, tackling tasks like text generation and summarization—some of the hardest challenges in NLP—meant months of reading papers, fine-tuning models, and throwing big compute at the problem. It was intense, resource-heavy work, but also incredibly rewarding when it all came together.

Fast forward to today, and advanced models like GPT-4 have changed the game entirely. What used to take me months can now be done in a few hours. But this progress comes with a huge challenge: sustainability. 🌍

Training and deploying large models consume a massive amount of energy, which got me thinking about alternatives. Enter smaller, more efficient language models like TinyLlama. Sure, they don’t match the big players in raw capability, but they open up a future where AI can be not just powerful but also responsible.

I’ve been getting more and more interested in how smaller models can help democratize AI while keeping sustainability in focus. The mantra isn’t “bigger is better” anymore—it’s “doing more with less.”

What’s next? I’m combining this small language model with Retrieval-Augmented Generation (RAG) and taking it to the cloud. Let’s see where this leads. The journey continues!

#AI #LLMs #TinyLlama #Sustainability
